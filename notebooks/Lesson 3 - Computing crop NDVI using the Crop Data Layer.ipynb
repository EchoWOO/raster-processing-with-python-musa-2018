{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining NDVI and CDL data\n",
    "\n",
    "In this lesson we'll combine our NDVI data with data from the [Cropland Data Layer](https://www.nass.usda.gov/Research_and_Science/Cropland/SARS1a.php) (CDL). The CDL is a land user/land cover raster at 30 meter resolution that classifies each pixel in a number of categories, including farmland for specific crops. The CDL puts out new CDL data each year; we'll be using data from the 2016 version.\n",
    "\n",
    "To start, let's take the GeoJSON of the crop area you pasted into Lesson 2 and paste it here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import shape\n",
    "\n",
    "#crop_area_geojson = \"\"\"\n",
    "#<PASTE GEOJSON FROM LESSON 2 HERE>\n",
    "#\"\"\"\n",
    "\n",
    "crop_area_geojson = \"\"\"\n",
    "<YOUR TEXT HERE>\n",
    "\"\"\"\n",
    "\n",
    "crop_area_json = json.loads(crop_area_geojson)['features'][0]['geometry']\n",
    "crop_area_ll = shape(crop_area_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CDL data lives in [Amazon S3](https://aws.amazon.com/s3) and is publicly accessable. We can access through HTTP, which `rasterio` can handle directly. In addition, because the GeoTIFF is a [Cloud Optimized GeoTIFF](http://www.cogeo.org/) (COG), only the data we need will be pulled from S3. This makes remote access a lot faster.\n",
    "\n",
    "First we'll need to figure out the CDL raster data's projection and reproject our area of interest into that projection. Then, we can use the `mask` method just like if the raster is on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "\n",
    "cdl_url = 'https://s3.amazonaws.com/geotrellis-test/cdl-geotiff/CDLS_2016_30m.tif'\n",
    "\n",
    "# Just read the CRS, so we can reproject our crop polygon to it.\n",
    "with rasterio.open(cdl_url) as ds:\n",
    "    cdl_crs = ds.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from shapely.geometry import mapping\n",
    "from musa import reproject_geom # From Lesson 1\n",
    "\n",
    "cdl_proj = pyproj.Proj(cdl_crs)\n",
    "lat_lng_proj = pyproj.Proj(init=\"epsg:4326\")\n",
    "\n",
    "crop_area = reproject_geom(crop_area_ll, lat_lng_proj, cdl_proj)\n",
    "\n",
    "with rasterio.open(cdl_url) as ds:\n",
    "    cdl_data, cdl_transform = mask(ds, [mapping(crop_area)], crop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's render this piece of the CDL. We'll use a `ListedColormap` in matplot lib to use the legend colors that are specified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "\n",
    "d = {1: \"#ffd300\",2: \"#ff2626\",3: \"#00a8e5\",4: \"#ff9e0c\",5: \"#267000\",6: \"#ffff00\",10: \"#70a500\",11: \"#00af4c\",12: \"#dda50c\",13: \"#dda50c\",14: \"#7fd3ff\",21: \"#e2007c\",22: \"#896354\",23: \"#d8b56b\",24: \"#a57000\",25: \"#d69ebc\",26: \"#707000\",27: \"#ad007c\",28: \"#a05989\",29: \"#700049\",30: \"#d69ebc\",31: \"#d1ff00\",32: \"#7f99ff\",33: \"#d6d600\",34: \"#d1ff00\",35: \"#00af4c\",36: \"#ffa5e2\",37: \"#a5f28c\",38: \"#00af4c\",39: \"#d69ebc\",41: \"#a800e5\",42: \"#a50000\",43: \"#702600\",44: \"#00af4c\",45: \"#b27fff\",46: \"#702600\",47: \"#ff6666\",48: \"#ff6666\",49: \"#ffcc66\",50: \"#ff6666\",51: \"#00af4c\",52: \"#00ddaf\",53: \"#54ff00\",54: \"#f2a377\",55: \"#ff6666\",56: \"#00af4c\",57: \"#7fd3ff\",58: \"#e8bfff\",59: \"#afffdd\",60: \"#00af4c\",61: \"#bfbf77\",63: \"#93cc93\",64: \"#c6d69e\",65: \"#ccbfa3\",66: \"#ff00ff\",67: \"#ff8eaa\",68: \"#ba004f\",69: \"#704489\",70: \"#007777\",71: \"#b29b70\",72: \"#ffff7f\",74: \"#b5705b\",75: \"#00a582\",76: \"#ead6af\",77: \"#b29b70\",81: \"#f2f2f2\",82: \"#9b9b9b\",83: \"#4c70a3\",87: \"#7fb2b2\",88: \"#e8ffbf\",92: \"#00ffff\",111: \"#4c70a3\",112: \"#d3e2f9\",121: \"#9b9b9b\",122: \"#9b9b9b\",123: \"#9b9b9b\",124: \"#9b9b9b\",131: \"#ccbfa3\",141: \"#93cc93\",142: \"#93cc93\",143: \"#93cc93\",152: \"#c6d69e\",176: \"#e8ffbf\",190: \"#7fb2b2\",195: \"#7fb2b2\",204: \"#00ff8c\",205: \"#d69ebc\",206: \"#ff6666\",207: \"#ff6666\",208: \"#ff6666\",209: \"#ff6666\",210: \"#ff8eaa\",211: \"#334933\",212: \"#e57026\",213: \"#ff6666\",214: \"#ff6666\",216: \"#ff6666\",217: \"#b29b70\",218: \"#ff8eaa\",219: \"#ff6666\",220: \"#ff8eaa\",221: \"#ff6666\",222: \"#ff6666\",223: \"#ff8eaa\",224: \"#00af4c\",225: \"#ffd300\",226: \"#ffd300\",227: \"#ff6666\",229: \"#ff6666\",230: \"#896354\",231: \"#ff6666\",232: \"#ff2626\",233: \"#e2007c\",234: \"#ff9e0c\",235: \"#ff9e0c\",236: \"#a57000\",237: \"#ffd300\",238: \"#a57000\",239: \"#267000\",240: \"#267000\",241: \"#ffd300\",242: \"#000099\",243: \"#ff6666\",244: \"#ff6666\",245: \"#ff6666\",246: \"#ff6666\",247: \"#ff6666\",248: \"#ff6666\",249: \"#ff6666\",250: \"#ff6666\",251: \"#ffd300\",252: \"#267000\",253: \"#a57000\",254: \"#267000ff\"}\n",
    "l = list(d.items())\n",
    "cmap = colors.ListedColormap(list(map(lambda x: x[1], l)))\n",
    "boundaries = [0] + list(map(lambda x: x[0], l))\n",
    "norm = colors.BoundaryNorm(boundaries, cmap.N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from musa import show_image\n",
    "show_image(cdl_data[0], cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the histogram of values to get a sense of the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from musa import show_histogram # From Lesson 2\n",
    "import numpy as np\n",
    "\n",
    "#show_histogram(np.ravel(cdl_data[~(cdl_data == 0)]), no_data_value=0)\n",
    "show_histogram(cdl_data, no_data_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not be very telling, as it doesn't show the names of the classes. We can compute the counts of each class by using the `np.unique` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from musa import cdl_values_to_crops\n",
    "\n",
    "flattened = np.ravel(cdl_data[~(cdl_data == 0)])\n",
    "values_arr, values_counts = np.unique(flattened, return_counts=True)\n",
    "\n",
    "# Zip the values together into a list of tuples\n",
    "values_to_counts = zip(values_arr, values_counts)\n",
    "\n",
    "# Use a list comprehension to transform crop IDs in to crop names\n",
    "crop_counts = [(cdl_values_to_crops[crop_id], count) for (crop_id, count) in values_to_counts]\n",
    "crop_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of [List Comprehensions](https://docs.python.org/3.6/tutorial/datastructures.html#list-comprehensionsa) above, which is a shorthand way to map over values in a collection.\n",
    "\n",
    "For further processing, we can use the [pandas](https://pandas.pydata.org/) library, a popular library in the Data Scientist's toolkit. It features a `DataFrame` type that allows you to work with data in a tabular fashion, much like a database table or a spreadsheet.\n",
    "\n",
    "Here we create a `DataFrame` from our crop count data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(crop_counts, columns=['crop','count'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to histograms numpy arrays, we can use matplotlib to plot a bar graph of our crop counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "df.plot.bar(x='crop', y='count', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That bar graph likely contains a lot of columns and is unreadable. To get around that, we can filter the `DataFrame` by a threshold to only look at classes that have a number of cells representing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 10000\n",
    "filtered = df[df['count'] > THRESHOLD]\n",
    "\n",
    "plt.figure()\n",
    "filtered.plot.bar(x='crop', y='count', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to play around with the `THRESHOLD` value to get a good set of classes.\n",
    "\n",
    "From that bar graph, pull out some crop names that we'll use for futher analysis, and replace the list below with your chosen crop names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_crops = ['Cotton',\n",
    "                'Corn',\n",
    "                'Safflower',\n",
    "                'Tomatoes',\n",
    "                'Almonds',\n",
    "                'Pistachios']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NDVI statistics per crop\n",
    "\n",
    "We'll now combine NDVI data and CDL data to inspect NDVI per crop type.\n",
    "\n",
    "Load up the landsat scenes, cropped to the area of interest, and compute the NDVI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "landsat_scene_name = \"LC08_L1TP_042035_20171022_20171107_01_T1\"\n",
    "\n",
    "red_path = os.path.join(\"/home/hadoop/data/\", \n",
    "                        \"{}_B4.TIF\".format(landsat_scene_name))\n",
    "ir_path = os.path.join(\"/home/hadoop/data/\", \n",
    "                       \"{}_B5.TIF\".format(landsat_scene_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with rasterio.open(red_path) as ds:\n",
    "    landsat_crs = ds.crs\n",
    "    \n",
    "landsat_proj = pyproj.Proj(landsat_crs)\n",
    "\n",
    "landsat_crop_area = reproject_geom(crop_area_ll, lat_lng_proj, landsat_proj)\n",
    "\n",
    "with rasterio.open(red_path) as ds:\n",
    "    (red_data, landsat_transform) = mask(ds, [mapping(landsat_crop_area)], crop=True)\n",
    "    \n",
    "with rasterio.open(ir_path) as ds:\n",
    "    (ir_data, _) = mask(ds, [mapping(landsat_crop_area)], crop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from musa import compute_ndvi\n",
    "ndvi = compute_ndvi(red_data[0], ir_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning the NDVI and CDL rasters\n",
    "\n",
    "In order to process rasters together, they need to be pixel aligned. Even though both CDL and Landsat data is at 30 meters, and is cropped to the same area, the pixel grids most likely do not align. In fact, the shape of the data most likely is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"{} == {} ? {}\".format(ndvi.shape, cdl_data[0].shape, ndvi.shape == cdl_data[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projects are also different. So we'll need to reproject one of the rasters into the other raster's space, and resample to align the pixel grids. \n",
    "\n",
    "Let's reproject the NDVI data to match the CDL data with the `reproject` method from `rasterio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasterio.warp import reproject\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warped_ndvi = np.zeros(cdl_data[0].shape)\n",
    "reproject(source=ndvi,\n",
    "          src_transform=landsat_transform,\n",
    "          src_crs=landsat_crs,\n",
    "          destination=warped_ndvi,\n",
    "          dst_transform=cdl_transform,\n",
    "          dst_crs=cdl_crs,\n",
    "          resampling=Resampling.bilinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including NDVI statistics in the DataFrame\n",
    "\n",
    "The code below calculates the min, max and mean NDVI values for each crop, and adds them to the `pandas` `DataFrame` that contains our crop counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from musa import crops_to_cdl_values\n",
    "\n",
    "def ndvi_stats(crop):\n",
    "    v = crops_to_cdl_values[crop]\n",
    "    masked_ndvi = np.ravel(warped_ndvi[(cdl_data[0] == v) & (~np.isnan(warped_ndvi))])\n",
    "    return pd.Series({'min': np.min(masked_ndvi),\n",
    "                      'max': np.max(masked_ndvi),\n",
    "                      'mean': np.mean(masked_ndvi)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df, df['crop'].apply(ndvi_stats)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More revealing than a table is what happens when we plot NDVI histograms per crop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_ndvi = np.min(warped_ndvi[~np.isnan(warped_ndvi)])\n",
    "max_ndvi = np.max(warped_ndvi[~np.isnan(warped_ndvi)])\n",
    "bins = np.linspace(min_ndvi, max_ndvi, 100)\n",
    "\n",
    "#target_crops = ['Corn']\n",
    "f, axarr = plt.subplots(len(target_crops), 1, figsize=(15,15), sharex=True)\n",
    "\n",
    "ndvis = []\n",
    "for crop in target_crops:\n",
    "    v = crops_to_cdl_values[crop]\n",
    "    ndvis.append(np.ravel(warped_ndvi[(cdl_data[0] == v) & (~np.isnan(warped_ndvi))]))\n",
    "    \n",
    "\n",
    "for i, crop in enumerate(target_crops):\n",
    "    v = crops_to_cdl_values[crop]\n",
    "    target_values = warped_ndvi[(cdl_data[0] == v) & (~np.isnan(warped_ndvi))]\n",
    "    axarr[i].hist(target_values, bins, label=crop)\n",
    "    axarr[i].set_title(crop)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data you likely see here indicates that some crop fields have a even spread of NDVI values, or have one grouping at a low NDVI value. However some crop fields have two groupings of NDVI values: places where the crop has been harvested, and places where the crop has grown and yet to be harvested*.\n",
    "\n",
    "*I'm not an agronomyst, so take my reading of the data with a large grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finding Harvested fields image segmentation\n",
    "\n",
    "Pick one crop that exhibits this behavior - two distinct \"peaks\" in the histogram. Replace the crop name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_name = \"Cotton\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [K-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering) to find the two cluster centers in our data. Since we only have 1 dimensional data, K-means clustering represents a [Jenks natural breaks](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization) seperation of the data.\n",
    "\n",
    "[scikit-learn](http://scikit-learn.org/stable/index.html) has a KMeans class that will let us perform k-means on our data easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We only want pixels of our crop, that are valid numbers.\n",
    "mask_cond = (~np.isnan(warped_ndvi)) &(cdl_data[0] == crops_to_cdl_values[crop_name])\n",
    "\n",
    "# Flatten out the data values, and reshape as needed for KMeans\n",
    "crop_ndvi = np.ravel(warped_ndvi[mask_cond]).reshape(-1, 1)\n",
    "\n",
    "# Run the unsupervised learning\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(crop_ndvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 2 cluster centers, we can find the value between them that represents the boundary point of the lower and higher groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "break_point = np.sum(kmeans.cluster_centers_) / 2\n",
    "break_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we are considering pixels containing our target crop classification with NDVI values less than or equal to the break_point to be \"harvested field\", and the similarly classified pixels with NDVI greater than the break point to be \"unharvested field\". \n",
    "\n",
    "Let's encode that information into two masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "\n",
    "def create_mask(condition):\n",
    "    unharvested_mask = np.zeros(warped_ndvi.shape, dtype='uint8')\n",
    "    unharvested_mask[mask_cond & condition] = 1\n",
    "    # Fill in holes in the data to try and create continuous regions.\n",
    "    return ndi.binary_fill_holes(unharvested_mask).astype('uint8')\n",
    "\n",
    "unharvested_mask = create_mask(warped_ndvi > break_point)\n",
    "harvested_mask = create_mask(warped_ndvi <= break_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the harvested and unharvested fields that have been segmented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_image(ndi.binary_fill_holes(unharvested_mask))\n",
    "show_image(ndi.binary_fill_holes(harvested_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[skimage](http://scikit-image.org/) is an image processing library that includes methods like creating contour lines. We use that functionality here to plot the contours of the fields only our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "def plot_contours(img_mask, title):\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(warped_ndvi, interpolation='none', cmap=cmap, clim=(-1.0, 1.0))\n",
    "\n",
    "    ax.axis('tight')\n",
    "\n",
    "    for n, contour in enumerate(find_contours(img_mask, 0)):\n",
    "        ax.plot(contour[:, 1], contour[:, 0], linewidth=4)\n",
    "\n",
    "plot_contours(unharvested_mask, \"Unharvested {} Fields\".format(crop_name))\n",
    "plot_contours(harvested_mask, \"Harvested {} Fields\".format(crop_name))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go head and write out our data as a shapefile. For this we'll use a library called [fiona](https://github.com/Toblerity/Fiona), a library for reading and writing vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fiona\n",
    "import rasterio.features\n",
    "\n",
    "schema = {\"geometry\": \"Polygon\", \"properties\": {\"value\": \"int\"}}\n",
    "\n",
    "# Create a raster with values:\n",
    "#  0 = Not containing target crop\n",
    "#  1 = Contains unharvested crop\n",
    "#  2 = Contains harvested crop\n",
    "\n",
    "mask_img = unharvested_mask.copy()\n",
    "mask_img[harvested_mask == 1] = 2\n",
    "\n",
    "features = []\n",
    "for (geom, value) in rasterio.features.shapes(mask_img, transform=cdl_transform):\n",
    "    if value != 0:\n",
    "        ll_geom = reproject_geom(shape(geom), cdl_proj, lat_lng_proj)\n",
    "        features.append({ 'geometry': mapping(ll_geom), \n",
    "                          'properties': { 'value': value} })\n",
    "\n",
    "# Write as a shapefile\n",
    "with fiona.open(\"/home/hadoop/data/fields.shp\", \"w\", \"ESRI Shapefile\",\n",
    "                crs=cdl_crs.data, schema=schema) as out_file:\n",
    "    out_file.writerecords(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write to GeoJSON using `json.dumps`. Here we set a property in the GeoJSON that will assign colors to the field boundaries in geojson.io. Go ahead and save off the GeoJSON, load the file up in geojson.io, and see if the fields match up with the satellite imagery basemap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write as a GeoJSON FeatureCollection\n",
    "def set_feature_type(f):\n",
    "    f['type'] = \"Feature\"\n",
    "    if f['properties']['value'] == 1:\n",
    "        f['properties']['fill'] = \"#00FF00\"\n",
    "    else:\n",
    "        f['properties']['fill'] = \"#FF0000\"\n",
    "    return f\n",
    "\n",
    "feature_collection = {\"type\": \"FeatureCollection\", \n",
    "                      \"features\": list(map(set_feature_type, features)) }\n",
    "with open(\"/home/hadoop/data/fields.geojson\", 'w') as f:\n",
    "    f.write(json.dumps(feature_collection, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoNotebook + GeoPySpark",
   "language": "python",
   "name": "geonotebook3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
